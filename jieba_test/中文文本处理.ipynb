{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 得到分词的词频大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "，\t13569\n",
      "。\t5644\n",
      "的\t5103\n",
      "　\t2756\n",
      "了\t2754\n",
      "“\t2494\n",
      "”\t2385\n",
      "我\t2004\n",
      "你\t1801\n",
      "：\t1754\n",
      "他\t1667\n",
      "她\t1299\n",
      "—\t1186\n",
      "是\t1182\n",
      "说\t1158\n",
      "鸿渐\t1051\n",
      "不\t993\n",
      "道\t933\n",
      "在\t926\n",
      "？\t865\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import jieba\n",
    "total = []\n",
    "with open('1.txt','rb') as f:\n",
    "    for line in f:\n",
    "#         print(line)\n",
    "        seg_list = jieba.cut(line.strip(),cut_all = False)\n",
    "        for word in seg_list:\n",
    "            total.append(word)\n",
    "c = Counter(total)\n",
    "for item in c.most_common(20):\n",
    "    print('{}\\t{}'.format(item[0],item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 为了提升分词精度，jieba支持自定义词典载入，可以自行构建一个词典文件，每个词语占据一行，格式为：词语 频率（可略） 词性（可略），然后用load_userdict()导入即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可以不用从源自定义词典文件改起，通过add_word(word, freq, tag)或del_word就可以达到同样效果。或者不希望载入自定义词典而希望让分词结果如你所愿，那么可以尝试建议频率：suggest_freq(word/tuple, tune = True/False)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里采用一个更加复杂的句子：“江州市长江大桥参加了长江大桥的通车仪式”。在这种场景下，需要大幅度提升“江大桥”的词频，这样才能保证当“市长”、“江大桥”出现在一起时，能够把“江大桥”这一人名给揪出来，但是这一频率不宜过大（没有一个绝对标准），否则会影响“长江大桥”的成词概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "江州/市/长江大桥/参加/了/长江大桥/的/通车/仪式\n"
     ]
    }
   ],
   "source": [
    "print('/'.join(jieba.cut('江州市长江大桥参加了长江大桥的通车仪式')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "江州/市长/江大桥/参加/了/长江大桥/的/通车/仪式\n"
     ]
    }
   ],
   "source": [
    "jieba.add_word('江大桥',freq = 20000,tag = None)\n",
    "print('/'.join(jieba.cut('江州市长江大桥参加了长江大桥的通车仪式')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 停用词的处理方法见另外一个文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 同义词的处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 建构同义词文件，每一行为一个同义词系列，如：孙悟空美猴王孙行者斗战胜佛放在一行内，每行的第一个词为希望分隔后呈现的词语，后几个词为第一个词的同义词，我这里采用Tab分隔，当然也可以用其他符号来加以分隔；接下来，构建字典，将初步分词的结果存进列表；最后将列表的值与字典进行匹配，若列表中的值等于字典中的同义词（Key），那么统一将这些列表中的值替换为字典中对应的值（Value）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn =dict()\n",
    "with open('synonyms.txt','r') as f:\n",
    "    for line in f:\n",
    "        for word in line.strip('\\n').split('\\t')[1:]\n",
    "            syn[unicode(word,'utf-8')] = unicode(line.strip('\\n').split('\\t')[0],'utf-8')\n",
    "with open('text.txt','r') as f:\n",
    "    for line in f:\n",
    "        seg_list = jieba.cut(line.strip(),cut_all=False)\n",
    "        for word in seg_list:\n",
    "            if word in syn.keys():\n",
    "                word = syn[word]\n",
    "                print(word)\n",
    "            else:\n",
    "                print(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
