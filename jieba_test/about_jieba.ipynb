{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 支持三种分词模式：\n",
    "- 精确模式，试图将句子最精确地切开，适合文本分析\n",
    "- 全模式，把句子中所有的可以成词的词语都扫描出来，速度非常快，但不能解决歧义\n",
    "- 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适用于搜索引擎分词\n",
    "- 支持繁体分词\n",
    "- 支持自定义词典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词 jieba.cut\n",
    "- 第一个参数为需要分词的字符串\n",
    "- cut_all参数用来控制是否采用全模式\n",
    "- jieba.cut_for_search方法接受一个参数：需要分词的字符串，\n",
    "    - 该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细\n",
    "   \n",
    "- jieba.cut以及jieba.cut_for_search返回的结构都是可迭代的generator，可以使用for循环来获得分词后得到的每一个词语，也可以用list(jieba.cut(..))转化为list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 2.866 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
      "Default Mode: 我/ 来到/ 北京/ 清华大学\n",
      "他, 来到, 了, 网易, 杭研, 大厦\n",
      "小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造\n"
     ]
    }
   ],
   "source": [
    "#encoding=utf-8\n",
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\",cut_all=True)\n",
    "print (\"Full Mode:\", \"/ \".join(seg_list)) #全模式\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\",cut_all=False)\n",
    "print (\"Default Mode:\", \"/ \".join(seg_list)) #精确模式\n",
    "\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\") #默认是精确模式\n",
    "print (\", \".join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\") #搜索引擎模式\n",
    "print (\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "- 对比可以看出全模式情况下，它会找出所有可以组成词的划分，而精确模式与其对比给出的答案就是只有一个\n",
    "- 关于HMM（hidden markov model:隐马尔可夫模型）参数，对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM为false:他/来到/了/网易/杭/研/大厦\n",
      "HMM为True:他/来到/了/网易/杭研/大厦\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "seg_list = jieba.cut('他来到了网易杭研大厦',HMM=False)\n",
    "print('HMM为false:'+'/'.join(seg_list))\n",
    "seg_list = jieba.cut('他来到了网易杭研大厦',HMM=True)\n",
    "print('HMM为True:'+'/'.join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - 所以一般情况下，使用cut方法，不用考虑HMM这个参数就可以，让它默认为True即可，让Viterbi算法为我们识别新词。HMM也能有效的解决中文中的歧义问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可以自己添加自定义字典\n",
    "- 开发者可以指定自己自定义的词典，以便包含jieba词库里没有的词。虽然jieba有新词识别能力，但是自行添加新词可以保证更高的正确率\n",
    "- 用法： jieba.load_userdict(file_name) # file_name为自定义词典的路径\n",
    "- 词典格式和dict.txt一样，一个词占一行；每一行分三部分，一部分为词语，另一部分为词频，最后为词性（可省略），用空格隔开"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "爱 v\n",
      "北京 ns\n",
      "天安门 ns\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words =pseg.cut(\"我爱北京天安门\")\n",
    "for w in words:\n",
    "    print (w.word,w.flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 并行分词\n",
    "- jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数jieba.disable_parallel() # 关闭并行分词模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "- 返回词语在原文的起始位置，注意参数只接受unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 永和\t\t start: 0 \t\t end:2\n",
      "word 服装\t\t start: 2 \t\t end:4\n",
      "word 饰品\t\t start: 4 \t\t end:6\n",
      "word 有限公司\t\t start: 6 \t\t end:10\n"
     ]
    }
   ],
   "source": [
    "result = jieba.tokenize(u'永和服装饰品有限公司')\n",
    "for tk in result:\n",
    "    print (\"word %s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获得前20词的权重大小 \n",
    "- 用jieba.analyse.extract_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('鸿渐', 0.21638643763446852)\n",
      "('辛楣', 0.12385522874320122)\n",
      "('小姐', 0.06859330337911916)\n",
      "('孙小姐', 0.06213265686414787)\n",
      "('柔嘉', 0.054676889357323594)\n",
      "('方鸿渐', 0.0496264917902045)\n",
      "('自己', 0.033501413977364065)\n",
      "('李梅亭', 0.028715200885500877)\n",
      "('唐小姐', 0.028197268553720612)\n",
      "('没有', 0.02806421719398845)\n",
      "('知道', 0.023341341308918614)\n",
      "('什么', 0.021301770841217118)\n",
      "('高松年', 0.021241381476945854)\n",
      "('先生', 0.02096255354677887)\n",
      "('太太', 0.020325655465768554)\n",
      "('汪太太', 0.019338798873938436)\n",
      "('女人', 0.019230220369696297)\n",
      "('可是', 0.019107288511088463)\n",
      "('他们', 0.01772965595927792)\n",
      "('赵辛楣', 0.017701151230788213)\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "file_name = '1.txt'\n",
    "content = open(file_name, 'rb').read()\n",
    "tags = jieba.analyse.extract_tags(content, topK=20,withWeight=True)\n",
    "for i in tags :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "- 去除某些词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import jieba\n",
    "from os import path\n",
    "\n",
    "d = path.dirname('__file__') # 获取当前文件的dir路径\n",
    "stopwords_path = 'stopwords1893.txt'  # 停用词表路径\n",
    "\n",
    "text_path = '1.txt' #《围城》的文本路径\n",
    "text = open(path.join(d, text_path),'rb').read()\n",
    "\n",
    "def RmStopWords(text):\n",
    "    mywordlist = []\n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    liststr=\"/ \".join(seg_list) # 添加切分符\n",
    "    f_stop = open(stopwords_path,encoding='utf-8')\n",
    "    try:\n",
    "        f_stop_text = f_stop.read()\n",
    "    finally:\n",
    "        f_stop.close( )\n",
    "    f_stop_seg_list=f_stop_text.split('\\n') # 停用词是每行一个，所以用/n分离\n",
    "    for myword in liststr.split('/'):\n",
    "        #对于每个切分的词都去停用词表中对比\n",
    "        if not(myword.strip() in f_stop_seg_list) and len(myword.strip())>1:\n",
    "            mywordlist.append(myword)\n",
    "    return ''.join(mywordlist) #返回一个字符串\n",
    "\n",
    "txt2 = RmStopWords(text)\n",
    "text_write = '2.txt'\n",
    "with open(text_write,'w') as f:\n",
    "    f.write(txt2)\n",
    "    print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('鸿渐', 0.28044557109396573)\n",
      "('辛楣', 0.16052138358382909)\n",
      "('小姐', 0.08889969422147594)\n",
      "('孙小姐', 0.0805264351515704)\n",
      "('柔嘉', 0.0708634590461673)\n",
      "('方鸿渐', 0.06431793962523721)\n",
      "('李梅亭', 0.03721606122568498)\n",
      "('唐小姐', 0.036544799985091365)\n",
      "('高松年', 0.027529689125849164)\n",
      "('太太', 0.026342871195032088)\n",
      "('汪太太', 0.02506386515607156)\n",
      "('女人', 0.024923143025037845)\n",
      "('赵辛楣', 0.022941407604874304)\n",
      "('学生', 0.022611225297624257)\n",
      "('李先生', 0.018797497809552442)\n",
      "('结婚', 0.017076181219597433)\n",
      "('明天', 0.016696820078620864)\n",
      "('东西', 0.016488028327530227)\n",
      "('仿佛', 0.014938565422774473)\n",
      "('也许', 0.014352840250177188)\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "file_name = '2.txt'\n",
    "content = open(file_name, 'rb').read()\n",
    "tags = jieba.analyse.extract_tags(content, topK=20,withWeight=True)#这句是关键\n",
    "for i in tags :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
